<pre class='metadata'>
Title: Completing the Design of Executors
Shortname: D2300
Revision: 0
Status: D
Group: WG21
Audience: SG1, LEWG
Editor: Michał Dominiak, Michał Dominiak <griwes@griwes.info>
URL: https://wg21.link/D2300
Metadata Order: Editor, This Version, Source, Issue Tracking, Project, Audience
Markup Shorthands: markdown yes
Toggle Diffs: no
No Abstract: yes
</pre>

<style>
pre {
  margin-top: 0px;
  margin-bottom: 0px;
}
.ins, ins, ins *, span.ins, span.ins * {
  background-color: rgb(200, 250, 200);
  color: rgb(0, 136, 0);
  text-decoration: none;
}
.del, del, del *, span.del, span.del * {
  background-color: rgb(250, 200, 200);
  color: rgb(255, 0, 0);
  text-decoration: line-through;
  text-decoration-color: rgb(255, 0, 0);
}
math, span.math {
  font-family: serif;
  font-style: italic;
}
ul {
  list-style-type: "— ";
}
blockquote {
  counter-reset: paragraph;
}
div.numbered, div.newnumbered {
  margin-left: 2em;
  margin-top: 1em;
  margin-bottom: 1em;
}
div.numbered:before, div.newnumbered:before {
  position: absolute;
  margin-left: -2em;
  display-style: block;
}
div.numbered:before {
  content: counter(paragraph);
  counter-increment: paragraph;
}
div.newnumbered:before {
  content: "�";
}
div.numbered ul, div.newnumbered ul {
  counter-reset: list_item;
}
div.numbered li, div.newnumbered li {
  margin-left: 3em;
}
div.numbered li:before, div.newnumbered li:before {
  position: absolute;
  margin-left: -4.8em;
  display-style: block;
}
div.numbered li:before {
  content: "(" counter(paragraph) "." counter(list_item) ")";
  counter-increment: list_item;
}
div.newnumbered li:before {
  content: "(�." counter(list_item) ")";
  counter-increment: list_item;
}
</style>

# Abstract # {#abstract}

This paper contains a self-contained design for an executors library for C++, based on the ideas in [[P0443R14]] and its companion papers.

## Code example ## {#design-code}

Using the facilities in this paper, an end user can write code such as this:

<pre highlight="c++">
using namespace std::execution;

// get a scheduler from somewhere, e.g. a thread pool
executor auto sch = get_thread_pool().scheduler();

// describe a chain of dependent work
sender auto begin = schedule(sch);
sender auto hi_again = then(begin, []{
    std::cout << "Hi again! Have an int.";
    return 13;
});
sender auto work = then(hi_again, [](int arg) { return arg + 42; });

// submit the work for execution on the pool
// block the current thread until its completion
// the return value is a tuple of values being the result of the sender chain
auto [i] = std::this_thread::sync_wait(work);
</pre>

See [[#design-factories]], [[#design-adapters]], and [[#design-algorithms]] for short explanations of the algorithms used in this code example.

## What this proposal is ## {#abstract-is}

This paper describes a design for a complete, self-contained library providing precise control for code execution on varying execution contexts in C++. This paper should be considered as a full design on its own, based heavily on prior papers that have been seen and
discussed by the committee.

Together with the paper, we are presenting a full, standalone, implementation of the design detailed in this paper, one that should satisfy the requirements of the prior papers that have been seen by the committee, but one that has also been validated to satisfy a
good part of the requirements of accelerator runtimes such as CUDA; our work to provide an efficient CUDA implementation of the sender/receiver model has directly driven most of the novel changes proposed here.

We are aiming at satisfying all the promises that have been given when the sender/receiver model has been proposed and adopted by SG1, most importantly that:

1. sender algorithms will allow for customization of their behavior based on the scheduler they are invoked for; and
2. the sender/receiver model is a replacement for two-way executors, which provided precise controls over places of execution of user code.

We believe that this aim has been achieved by the design proposed here.

## What this proposal is **not** ## {#abstract-is-not}

This paper is not a patch on top of [[P0443R14]]; we are not asking to update the existing paper, we are asking to retire it in favor of this paper, which is already self-contained; any example code within this paper can be written in Standard C++, without the need
to standardize any further facilities.

This paper is not an alternative design to [[P0443R14]]; rather, we have taken the design in the current executors paper, and applied targetted fixes to allow it to fulfill the promises of the sender/receiver model, as well as provide all the facilities we consider
essential when writing user code using standard execution concepts; we have also applied the guidance of removing one-way executors from the paper entirely, and instead provided an algorithm based around senders that serves the same purpose.

## What are the major design changes compared to P0443? ## {#abstract-compare}

1. This proposal does not propose any specific type erasure facilities; it does, however, discuss type erasure to an extend in [[#design-dispatch]].
2. Properties are not included in this paper; we see them as a possible future extension, if the committee gets more comfortable with them.
3. This paper does not include a specific thread pool implementation, per prior committee direction to propose it separately.
4. We have implemented the SG1 direction to remove executors and base all of the proposed functionalities on senders and schedulers.
5. Senders now advertise what scheduler, if any, they are bound to, and they also advertise whether they will execute in a strictly lazy way (with no work being submitted for execution until `execution::start` is called on an operation state they produced) or not.
6. The places of execution of user code in P0443 weren't precisely defined, whereas they are in this paper. See [[#design-propagation]].
7. P0443 did not propose a suite of algorithms necessary for writing sender code; this paper does. See [[#design-factories]], [[#design-adapters]], and [[#design-algorithms]].
8. P0443 did not specify the semantics of variously qualified `connect` overloads; this paper does. See [[#design-fork]].

# Revision history # {#revisions}

## R0 ## {#r0}

Initial revision, still in progress.

# Proposed design - introduction # {#design-intro}

The following four sections describe the entirety of the proposed design. [[#design-intro]] describes the conventions used through the rest of the design sections, as well as a code example illustrating how we envision code will be written using this proposal.

[[#design-user]] describes all the functionality from the perspective we intend for users: it describes what the various concepts they will interact with, and what their programming model is. [[#design-implementer]] describes the machinery that allows for
that programming model to function, and the information contained there is necessary for people implementing senders and sender algorithms (including the standard library ones) - but is not necessary for being able to use senders productively.

Finally, [[#design-api]] discusses API decisions and questions that we believe reflect on the code that will need to be written to implement and use senders, but not on the programming model of senders in general.

Note: none of the algorithm names proposed here are names that we are particularly attached to; consider the names of the algorithms to be reasonable placeholders that can freely be changed, should the committee want to do so.

## Conventions ## {#design-conventions}

The following conventions are used throughout the design section:

  1. Universal references and explicit calls to `std::move`/`std::forward` are omitted in code samples and signatures for simplicity; assume universal references and perfect forwarding unless stated otherwise.
  2. The namespace proposed in this paper is the same as in [[P0443R14]]: `std::execution`; however, for brevity, the `std::` part of this name is omitted. When you see `execution::foo`, treat that as `std::execution::foo`.

# Proposed design - user side # {#design-user}

## Execution contexts describe the place of execution ## {#design-contexts}

An <dfn>execution context</dfn> is a resource that represents the *place* where execution will happen. This could be a concrete resource - like a specific thread pool object, or a GPU - or a more abstract one, like the current thread of execution. Execution contexts
don't need to have a representation in code; they are simply a term describing certain properties of execution of a function.

## Schedulers represent execution contexts ## {#design-schedulers}

A <dfn>scheduler</dfn> is a lightweight handle to an *execution context*, which allows describing work that will be executed on an execution agent belonging to that context. Since execution contexts don't necessarily manifest in C++ code, it's not possible to program
directly against their API. A scheduler is a solution to that problem: the scheduler concept is defined by a single operation, `schedule`, which creates a description of work on its associated execution context, materialized in the form of a sender.

<pre highlight="c++">
execution::scheduler auto sch = get_thread_pool().scheduler();
execution::sender auto snd = execution::schedule(sch);
// snd is a sender (see below) describing the creation of a new execution resource
// on the execution context associated with sch
</pre>

## Senders describe work ## {#design-senders}

A <dfn>sender</dfn> is an object that describes work. Senders are similar to futures in existing asynchrony designs, but unlike futures, the work that is being done to arrive at the values they will *send* is also directly described by the sender object itself. A
sender is said to <dfn>send</dfn> some values if a receiver connected (see [[#design-connect]]) to that sender will eventually *receive* said values.

The primary defining operation of senders is [[#design-connect]]; this function, however, is not a user-facing API; it is used to facilitate communication between senders and various sender algorithms (or other senders), but end user code is not expected to invoke
it directly. The way user code is expected to interact with senders is by using said sender algorithms; the opening example of the design section already contains a sample of sender algorithms, and the ones we are proposing as a part of this paper are described in
[[#design-factories]], [[#design-adapters]], and [[#design-algorithms]]. Here is how a user can ensure that work is started (and will eventually complete if no errors happen), but without waiting for it to finish:

<pre highlight="c++">
execution::scheduler auto sch = get_thread_pool().scheduler();
execution::sender auto snd = execution::schedule(sch);
execution::sender auto cont = execution::then(snd, []{
    std::fstream file{ "result.txt" };
    file << compute_result;
});

execution::submit(cont);
// at this point, the work described by \`cont\` has been submitted to the thread pool
</pre>

## Senders may be bound to schedulers ## {#design-propagation}

One of the less clear aspects of [[P0443R14]] has been the set of requirements for the <i>place of execution</i> of any given piece of code. This underspecification is problematic, especially in case of systems where not all execution agents are created equal,
and not all functions can be run on all execution agents. Having precise control over the execution context used for any given function call being submitted is very important on such systems, and the users of standard execution facilities will expect to be albe
to express such (binding) requirements.

Such precise control was present in the two-way execution API, but it has so far been missing from the senders design. There has been a proposal ([[P1897R3]]) to provide a number of algorithms that would enforce certain rules on the places of execution
of the work described by a sender, but we have found those algorithms to be insufficient for achieving the best performance on all platforms that are of interest to us. The implementation strategies that we are aware of result in one of the following situations:

  1. trying to submit work to CPU execution contexts (e.g. a thread pool) from an accelerator (e.g. a GPU), which assumes that the accelerator threads of execution are as capable as the CPU threads of execution (which they aren't); or
  2. forcibly interleaving two adjacent execution graph nodes that are both executing on an accelerator with glue code that runs on the CPU; this operation is prohibitively expensive on runtimes such as CUDA.

Neither of these implementation strategies is acceptable for accelerator runtimes. Therefore, in addition to the `on` algorithm from [[P1897R3]], we are proposing to add a standardized way for senders to advertise what scheduler (and by extension - what execution
context) any work (save for explicit transitions) attached to them will execute on. Any given sender <b>may</b> have an <dfn>underlying scheduler</dfn>. When further work is attached to that sender by invoking sender algorithms, that work will also execute on the
underlying scheduler. For instance:

<pre highlight="c++">
execution::scheduler auto sch = new_thread_scheduler{};
// sch is a scheduler that starts work on a new thread

execution::sender auto initial = execution::schedule(sch);
execution::sender auto next = execution::then(initial, []{
    std::cout << "First continuation" << std::endl;
});
execution::sender auto last = execution::then(next, []{
    std::cout << "Second continuation" << std::endl;
});

std::this_thread::sync_wait(last);
// both continuations will run on the same thread created by the scheduler
</pre>

There exists a function to retrieve the [=underlying scheduler=] from a sender, called `get_underlying_scheduler`. At the end of the example above, `execution::get_underlying_scheduler(last)` returns an object equivalent to `sch`. Calling this function on a sender
that does not have an underlying scheduler is ill-formed. If a scheduler advertises its underlying scheduler in this way, that sender <b>must</b> ensure that it [=send|sends=] its values on an execution agent belonging to an execution context represented by a
scheduler returned from this function.

## Senders advertise if they are lazy ## {#design-laziness}

It is sometimes necessary for the correctness of an algorithm to know whether calling a sender algorithm on a sender can cause work to start without explicitly triggering it (for instance by calling [[#design-adapter-ensure_started]]). A consumer of a sender can
check whether a sender guarantees that work will not be submitted before `ensure_started` or a similar function is called by checking the value of `execution::sender_traits<Sender>::is_strictly_lazy`. A sender <b>must</b> strictly adhere to this value.

Only senders that have an [=underlying scheduler=] are allowed to set the above value to `false`. If a sender does not have an underlying scheduler, that means there is not enough information for it to actually start work on its own, and that information must be
provided later, for instance by passing it to `execution::on`.

A sender can be stripped off of its underlying scheduler, and by consequence - turned into a strictly lazy sender - by calling [[#design-adapter-unschedule]].

## Execution context transitions are explicit ## {#design-transitions}

[[P0443R14]] does not contain any mechanisms for performing an execution context transition. The only algorithm that can create a sender that will move execution to a specific* execution context is `execution::schedule`, which does not take a predecessor, which
means that there's no way to construct sender chains that traverse different execution contexts. This is necessary to fulfill the promise of senders being able to replace two-way executors, which had this capability.

We propose that, for senders advertising their [=underlying scheduler=], all execution context transitions <b>must</b> be explicit; running user code anywhere but where they defined it to run <b>must</b> be considered a bug.

We propose a new user-facing algorithm, `execution::transfer`, for performing transitions from one execution context to another:

<pre highlight="c++">
execution::scheduler auto sch1 = ...;
execution::scheduler auto sch2 = ...;

execution::sender auto snd1 = execution::schedule(sch1);
execution::sender auto then1 = execution::then(snd1, []{
    std::cout << "I am running on sch1!\n";
});

execution::sender auto snd2 = execution::transfer(then1, sch2);

execution::sender auto then2 = execution::then(snd2, []{
    std::cout << "I am running on sch2!\n";
});

std::this_thread::sync_wait(then2);
</pre>

## Senders are forkable ## {#design-fork}

Any non-trivial program will eventually want to fork a chain of senders into independent streams of work; an incoming event to a middleware system may be required to trigger events on more than one downstream system, for instance. This requires that senders provide
well defined mechanisms for making sure that attaching multiple continuations to a sender is possible and correct.

There are two parts of what we are proposing in this area. One is a semantic requirement on both senders and sender algorithms: if you invoke a sender algorithm with an lvalue sender, that sender must remain valid after such an algorithm invocation. If your algorithm
accepts an lvalue sender, it must not invalidate the lvalue it has received. It must be possible to invoke sender algorithms with rvalue senders, but lvalues must not be invalidated other than by turning them in rvalues ("moving" them).

The consequences of the above requirement are as follows:

 * single-shot senders should only work with rvalue-qualified overloads of sender algorithms;
 * multi-shot senders should work with at least some lvalue-qualified algorithms; and
 * multi-shot senders can still have operations that require moving them into an algorithm call.

To facilitate attaching multiple continuations to the same sender multiple times regardless of whether it is single-shot or multi-shot, we are also proposing a new sender algorithm: `split`.

<pre highlight=c++>
auto some_algorithm(execution::sender auto predecessor) {
    // here, we can test whether the predecessor is multi-shot
    // (for the purposes of the algorithms being used in this function)

    // but we can also turn the predecessor into a multi-shot sender:

    execution::sender auto multi_shot = split(predecessor);
    // multi_shot is guaranteed to be multi-shot
}
</pre>

## Senders are joinable ## {#design-join}

Similarly to how it's hard to write a complex program that will eventually want to fork sender chains into independent streams, it's also hard to write a program that does not want to eventually create join nodes, where multiple independent streams of execution are
merged into a single one in an asynchronous fashion. We are proposing two algorithms that deal with the common use cases for such merging: `when_all` and `when_any`.

Both of these algorithms are very similar; they differ primarily in when they complete, and what values they send:

The sender returned from `when_all` completes when the last of the predecessor senders completes. It [=send|sends=] a pack of values, where the elements of said pack are the values sent by the predecessor senders, in order.

The sender returned from `when_any` completes when the first of the predecessor senders completes. It [=send|sends=] the value that has been sent by that first predecessor to complete.

We propose that when all the predecessor senders to both of these algorithms do not have an [=underlying scheduler=], there is no additional information that must be passed into `when_all` and `when_any`, and they should both return senders that do not have an
underlying scheduler. A similar route could be taken for senders bound to schedulers, but we believe that this is suboptimal. If the joining algorithm return senders that are detached from any schedulers that have to be explicitly transitioned onto one, they may
result in a loss of efficiency, because additional control and data transfers may need to happen in such case. Therefore, we propose that when at least one of the sender arguments to the joining algorithms has an underlying scheduler, a user must also provide an
explicit scheduler argument, which describes the scheduler that the returned sender will be bound to.

## Proposed set of user-facing sender factories ## {#design-factories}

A <dfn>sender factory</dfn> is a function which creates new senders, without requiring a predecessor sender.

### `execution::schedule` ### {#design-factory-schedule}

<pre highlight="c++">
execution::sender auto schedule(
    execution::scheduler auto scheduler
);
</pre>

Returns a sender describing the start of a task graph on the provided scheduler. See [[#design-schedulers]].

<pre highlight="c++">
execution::scheduler auto sch1 = get_system_thread_pool().scheduler();

execution::sender auto snd1 = execution::schedule(sch1);
// snd1 describes the creation of a new task on the system thread pool
</pre>

### `execution::just` ### {#design-factory-just}

<pre highlight="c++">
execution::sender auto just(
    auto ...values
);
</pre>

Returns a sender with no [=underlying scheduler=], which [=send|sends=] copies of the provided values.

TODO: example

### `execution::just_on` ### {#design-factory-just_on}

<pre highlight="c++">
execution::sender auto just_on(
    execution::scheduler auto scheduler,
    auto ...values
);
</pre>

Returns a sender whose [=underlying scheduler=] is the provided scheduler, which [=send|sends=] copies of the provided values.

<pre highlight="c++">
execution::sender auto vals = execution::just_on(get_system_thread_pool().scheduler(),
    1, 2, 3
);
execution::sender auto snd = execution::then(pred, [](auto... args) {
    std::print(args..);
});
// when snd is executed, it will print "123"
</pre>

This algorithm is included, as it greatly simplifies lifting values into senders.

## Proposed set of user-facing sender adapters ## {#design-adapters}

A <dfn>sender adapter</dfn> is a function which accepts one or more senders, and possibly other algorithms, and returns a sender, whose completion is related to the sender arguments it has received.

### `execution::transfer` ### {#design-adapter-transfer}

<pre highlight="c++">
execution::sender auto transfer(
    execution::sender auto predecessor,
    execution::scheduler auto scheduler
);
</pre>

Returns a sender describing the transition from the execution agent of the predecessor to the execution agent of the target scheduler. See [[#design-transitions]].

<pre highlight="c++">
execution::scheduler auto cpu_sched = get_system_thread_pool().scheduler();
execution::scheduler auto gpu_sched = cuda::scheduler();

execution::sender auto cpu_task = execution::schedule(cpu_sched);
// cpu_task describes the creation of a new task on the system thread pool

execution::sender auto gpu_task = execution::transfer(cpu_task, gpu_sched);
// gpu_task describes the transition of the task graph described by cpu_task to the gpu
</pre>

### `execution::then` ### {#design-adapter-then}

<pre highlight="c++">
execution::sender auto then(
    execution::sender auto predecessor,
    std::invocable<<i>values-sent-by(predecessor)</i>...> function
);
</pre>

Returns a sender describing the task graph described by the predecessor, with an added node of invoking the provided function with the values [=send|sent=] by the predecessor as arguments.

<pre highlight="c++">
execution::sender auto pred = get_predecessor();
execution::sender auto snd = execution::then(pred, [](auto... args) {
    std::print(args..);
});
// snd describes the work described by pred
// followed by printing all of the values sent by pred
</pre>

This algorithm is included, as it is necessary for writing any sender code that actually performs a useful function.

### `execution::on` ### {#design-adapter-on}

<pre highlight="c++">
execution::sender auto on(
    execution::scheduler auto sched,
    execution::sender auto snd
);
</pre>

TODO

### `execution::unschedule` ### {#design-adapter-unschedule}

<pre highlight="c++">
execution::sender unschedule(
    execution::sender auto snd
);
</pre>

Returns a sender which [=send|sends=] values equivalent to values sent by the provided sender, but does not have an [=underlying scheduler=].

TODO: example

### `execution::bulk` ### {#design-adapter-bulk}

<pre highlight="c++">
execution::sender auto bulk(
    execution::sender auto predecessor,
    std::integral auto size,
    invocable&lt;decltype(size), <i>values-sent-by(predecessor)</i>...> function
);
</pre>

Returns a sender describing the task of invoking the provided function with the values [=send|sent=] by the predecessor for every index in the provided shape.

In this paper, only integral types satisfy the concept of a shape, but future papers will explore bulk shapes of different kinds in more detail.

XXX TODO: provide an example

### `execution::split` ### {#design-adapter-split}

<pre highlight="c++">
execution::sender auto split(execution::sender auto sender);
</pre>

If the provided sender is a multi-shot sender, returns that sender. Otherwise, returns a multi-shot sender which sends values equivalent to the values sent by the provided sender. See [[#design-fork]].

### `execution::when_all` ### {#design-adapter-when_all}

<pre highlight="c++">
execution::sender auto when_all(
    execution::scheduler auto sched,
    execution::sender auto ...predecessors
);

execution::sender auto when_all(
    execution::sender auto ...predecessors
) requires (!<i>have-underlying-schedulers(predecessors...)</i>);
</pre>

Returns a sender which completes once all of the predecessor senders have completed. The values send by this sender are the values sent by each of the predecessor, in order of the arguments passed to `when_all`.

For the first overload, the returned sender will complete on an execution agent obtained from the provided scheduler. For the second overload, the returned sender shall not have an underlying scheduler.

See [[#design-join]].

<pre highlight="c++">
execution::scheduler auto sched = get_thread_pool().scheduler();

execution::sender auto sends_1 = ...;
execution::sender auto sends_abc = ...;

execution::sender auto both = execution::when_all(sched,
    sends_1,
    sends_abc
);

execution::sender auto final = execution::then(both, [](auto... args){
    std::cout << std:;format("the two args: {}, {}", args...);
});
// when final executes, it will print "the two args: 1, abc"
</pre>

### `execution::when_any` ### {#design-adapter-when_any}

<pre highlight="c++">
execution::sender auto when_any(
    execution::scheduler auto sched,
    execution::sender auto ...predecessors
);

execution::sender auto when_any(
    execution::sender auto ...predecessors
) requires (!<i>have-underlying-schedulers(predecessors...)</i>);
</pre>

Returns a sender which completes once any of the predecessor senders has completed. The value sent by this sender is the value sent by the first of the predecessors that completes.

For the first overload, the returned sender will complete on an execution agent obtained from the provided scheduler. For the second overload, the returned sender shall not have an underlying scheduler.

See [[#design-join]].

<pre highlight="c++">
execution::scheduler auto sched = get_thread_pool().scheduler();

execution::sender auto sends_1 = ...;
execution::sender auto sends_abc = ...;

execution::sender auto either = execution::when_any(sched,
    sends_1,
    sends_abc
);

execution::sender auto final = execution::then(either, [](auto arg){
    std::cout << std:;format("the only arg: {}", arg);
});
// when final executes, it will print one of the following:
// * "the only arg: 1", or
// * "the only arg: abc"
</pre>

### `execution::ensure_started` ### {#design-adapter-ensure_started}

<pre highlight="c++">
execution::sender auto ensure_started(
    execution::sender auto sender
);
</pre>

Once `ensure_started` returns, it is known that the provided sender has been [=connect|connected=] and `start` has been called on the resulting operation state (see [[#design-states]]); in other words, the work described by the provided sender has been submitted
for execution on the appropriate execution contexts. Returns a sender which completes when the provided sender completes and sends values equivalent to those of the provided sender.

XXX TODO: provide an example

## Proposed set of user-facing sender algorithms ## {#design-algorithms}

A <dfn>sender algorithm</dfn> is a function that accepts one or more senders, but does not return a sender.

### `execution::submit` ### {#design-adapter-submit}

<pre highlight="c++">
void auto submit(
    execution::sender auto sender
);
</pre>

Like `ensure_started`, but does not return a value; if the provided sender sends an error instead of a value, `std::terminate` is called.

### `std::this_thread::sync_wait` ### {#design-algorithm-sync_wait}

<pre highlight="c++">
auto sync_wait(
    execution::sender auto sender
) -> std::tuple<<i>values-sent-by(sender)</i>>;
</pre>

`std::this_thread::sync_wait` is an algorithm that submits the work described by the provided sender for execution, similarly to `ensure_started`, execept that it blocks <b>the current `std::thread` or thread of `main`</b> until the work is completed, and returns
a tuple of values that were sent by the provided sender on its completion of work. Where [[#design-factory-schedule]] and [[#design-factory-just_on]] are meant to <i>enter</i> the domain of senders, `sync_wait` is meant to <i>exit</i> the domain of senders,
retrieving the result of the task graph.

XXX TODO: provide an example

Note: Notice that this function is specified inside `std::this_thread`, and not inside `execution`. This is because `sync_wait` has to block the <i>current</i> execution agent, but determining what the current execution agent is is not reliable. Since the standard
does not specify any functions on the current execution agent other than those in `std::this_thread`, this is the flavor of this function that is being proposed. If C++ ever obtains fibers, for instance, we expect that a variant of this function called
`std::this_fiber::sync_wait` would be provided. We also expect that runtimes with execution agents that use different synchronization mechanisms than `std::thread`'s will provide their own flavors of `sync_wait` as well (assuming their execution agents have the means
to block in a non-deadlock manner).

## `execution::execute` ## {#design-execute}

In addition to the three categories of functions presented above, we also propose to include a convenience function for fire-and-forget eager one-way submission of an invocable to a scheduler, to fullfil the role of one-way executors from P0443.

<pre highlight="c++">
void execution::execute(
    execution::schedule auto sched,
    std::invocable<void> auto fn
);
</pre>

Submits the provided function for execution on the provided scheduler, as-if by:

<pre highlight="c++">
auto snd = execution::schedule(sched);
auto work = execution::then(snd, fn);
execution::submit(work);
</pre>

TODO: example

# Proposed design - implementer side # {#design-implementer}

## Senders provide optimization opportunities ## {#design-fusion}

XXX TODO: describe "kernel" fusion better here

Because senders fundamentally *describe* work, instead of describing or representing submission of said work to an execution context, they provide an opportunity for fusing multiple operations in a sender chain together.

## Receivers serve as glue between senders ## {#design-receivers}

A <dfn>receiver</dfn> is a callback that supports more than one channel. In fact, it supports three of them:

* `set_value`, which is the moral equivalent of an `operator()` or a function call, which signals successful completion of its predecessors;
* `set_error`, which signals that an error has happened during scheduling of the current work, executing the current work, or at some earlier point in the sender chain; and
* `set_done`, which signals that the *predecessor* requests that any work that has not started yet should not start, because it is not needed (this is cancellation, but propagating from the root of the execution tree down to its leaf nodes, as opposed to propagating
    from the leaf nodes up to the root - so **not** `some_sender.cancel()`).

Exactly one of these channels must be successfully (i.e. without an exception being thrown) invoked on a receiver before it is destroyed; if a call to `set_value` failed with an exception, either `set_error` or `set_done` must be invoked on the same receiver. These
requirements are know as the <dfn>receiver contract</dfn>.

While the receiver interface may look novel, it is in fact very similar to the interface of `std::promise`, which provides the first two signals as `set_value` and `set_error`, and it's possible to emulate the third channel with lifetime management of the promise.

Receivers are not a part of the end-user-facing API of this proposal; they are necessary to allow unrelated senders communicate with each other, but the only users who will interact with receivers directly are authors of senders.

Receivers are what is passed as the second argument to [[#design-connect]].

## Operation states represents work ## {#design-states}

An <dfn>operation state</dfn> is an object that represents work. Unlike senders, it is not a chaining mechanism; instead, it is a concrete object that packages the work described by a full sender chain, ready to be executed. An operation state is neither movable nor
copyable, and its interface consists of a single operation: `start`, which serves as the submission point of the work represented by a given operation state.

Operation states are not a part of the user-facing API of this proposal; they are necessary for implementing algorithms like `ensure_started` and `std::this_thread::sync_wait`, and the knowledge of them is necessary to implement senders, so the only users who will
interact with operation states directly are authors of senders and authors of sender algorithms.

The return value of [[#design-connect]] must satisfy the operation state concept.

## `execution::connect` ## {#design-connect}

`execution::connect` is a customization point which <dfn lt="connect">connects</dfn> senders with receivers, resulting in an operation state that will ensure that the [=receiver contract=] of the receiver passed to connect will be fulfilled.

<pre highlight="c++">
execution::sender auto snd = <i>some predecessor sender</i>;
execution::receiver auto rcv = <i>some receiver</i>;
execution::operation_state auto state = execution::connect(snd, rcv);

execution::start(state);
// at this point, it is guaranteed that the work represented by state has been submitted
// to an execution context, and that execution context will eventually fulfill the
// receiver contract of rcv

// operation states are not movable, and therefore this operation state object must be
// kept alive until the operation finishes
</pre>

## Sender algorithms are customizable ## {#design-customization}

Senders being able to be bound to schedulers, and being able to advertise what their [=underlying scheduler=] is, fulfills one of the promises of senders: that of being able to customize an implementation of an algorithm based on what scheduler it will execute on.

The simple way to provide customizations for functionS like `then`, that is for [=sender adapter|sender adapters=] and [=sender algorithm|algorithms=], is to follow the customization scheme that has been adopted for C++20 Ranges library; to do that, we would define
the expression `execution::then(sender, invocable)` to be equivalent to:

  1. `sender.then(invocable)`, if that expression is well formed; otherwise
  2. `then(sender, invocable)`, performed in a context where this call always performs ADL, if that expression is well formed; otherwise
  3. a default implementation of `then`, which returns a sender adapter, and then define the exact semantics of said adapter.

However, this definition is problematic. Imagine another sender algorithm, `bulk`, which is a structured abstraction for a loop over an index space. Its default implementation is just a for loop. However, for accelerator runtimes like CUDA, we would like operations
like `bulk` to have specialized behavior, which invokes a kernel of more than one thread (with its size defined by the call to `bulk`); therefore, we would like to customize `bulk` for CUDA senders to achieve this. However, there's no reason for CUDA kernels to
necessarily customize the `then` algorithm, as the generic implementation is perfectly sufficient. This creates a problem, though; consider the following snippet:

<pre highlight="c++">
execution::scheduler auto cuda_sch = cuda_scheduler{};

execution::sender auto initial = execution::schedule(cuda_sch);
// the type of initial is a type defined by the cuda_scheduler
// let's call it cuda::schedule_sender&lt;>

execution::sender auto next = execution::then(cuda_sch, []{ return 1; });
// the type of next is a standard-library implementation-defined sender adapter
// that wraps the cuda sender
// let's call it execution::then_sender_adapter&lt;cuda::schedule_sender&lt;>>

execution::sender auto kernel_sender = execution::bulk(next, shape, [](int i){ ... });
</pre>

How can we specialize the `bulk` algorithm for our wrapped `schedule_sender`? Well, here's one possible approach, taking advantage of ADL (and the fact that the definition of "associated namespace" also recursively enumerates the associated namespaces of all template
parameters of a type):

<pre highlight="c++">
namespace cuda::for_adl_purposes {
template&lt;typename... SentValues>
class schedule_sender {
    execution::operation_state auto connect(execution::receiver auto rcv);
    execution::scheduler auto get_scheduler() const;
};

execution::sender auto bulk(
    execution::sender auto && predecessor,
    execution::shape auto && shape,
    invocable<<i>sender-values(predecessor)</i>> auto && fn)
{
    // return a cuda sender representing a bulk kernel launch
}
}
</pre>

However, if the predecessor is not just a `then_sender_adapter` like in the example above, but another sender that overrides `bulk` by itself, as a member function, because its author believes they know an optimization for bulk - the specialization above will no
longer be selected, because a member function of the first argument is a better match than the ADL-found overload.

This means that well-meant specialization of algorithms on senders that are entirely scheduler-agnostic can have negative consequences; the scheduler-specific specialization - which is essential for good performance on platforms providing specialized ways to launch
certain algorithms - would not be selected in such cases. But it's really the scheduler that should control the behavior of algorithms when a non-default implementation exists, not the sender. Senders merely describe work; schedulers, however, are the handle to the
runtime that will eventually execute said work, and should thus have the final say in *how* the work is going to be executed.

Therefore, we are proposing the following customization scheme: the expression `execution::<algorithm>(sender, args...)`, for any given algorithm (accepting a sender as its first argument), should be equivalent to:

  1. `get_scheduler(sender).<algorithm>(sender, args...)`, if that expression is well-formed; otherwise
  2. `sender.<algorithm>(args...)`, if that expression is well-formed; otherwise
  3. `<algorithm>(sender, args...)`, performed in a context where this call always performs ADL, if that expression is well formed; otherwise
  4. a default implementation, if there exists a default implementation of the given algorithm.

For algorithms which accept concepts other than `sender` as their first argument, we propose that the customization scheme remains as it has been in [[P0443R14]] so far.

TODO: more example

## Execution context transitions are two-step ## {#design-transition-details}

Because `execution::transfer` takes a sender as its first argument, it is not actually directly customizable by the target scheduler. This is by design: the target scheduler may not know how to transition <i>from</i> a scheduler such as a CUDA scheduler;
transitioning away from a GPU in an efficient manner requires making runtime calls that are specific to the GPU in question, and the same is usually true for other kinds of accelerators too (or for scheduler running on remote systems). To avoid this problem,
specialized schedulers like the ones mentioned here can still hook into the transition mechanism, and inject a sender which will perform a transition to the regular CPU execution context, so that any sender can be attached to it.

This, however, is a problem: because customization of algorithms must be controlled by the scheduler they will run on (see [[#design-customization]]), the type of the sender returned from `transfer` must be controllable by the target scheduler. Besides, the target
scheduler may itself represent a specialized execution context, which requires additional work to be performed to transition <i>to</i> it. GPUs and remote node schedulers are once again good examples of such schedulers: executing code on their execution contexts
requires making runtime API calls for work submission, and quite possibly for the data movement of the values being sent by the predecessor passed into `transfer`.

To allow for such customization from both ends, we propose the inclusion of a secondary transitioning algorithm, called `schedule_from`. This algorithm is a form of `schedule`, but takes an additional, second argument: the predecessor sender. This algorithm is not
meant to be invoked manually by the end users; they are always supposed to invoke `transfer`, to ensure that both schedulers have a say in how the transitions are made. Any predecessor scheduler that specializes `transfer(snd, sch)` shall ensure that the
return value of their customization is equivalent to `schedule_from(sch, snd2)`, where `snd2` is a successor of `snd` that sends values equivalent to those sent by `snd`.

The default implementation of `transfer(snd, sched)` is `schedule_from(sched, snd)`.

# Proposed design - API specifics # {#design-api}

## Senders are pipeable ## {#design-pipelines}

All sender algorithms that take a single predecessor that we are proposing here take the predecessor as the first argument. This is not an accident; in line with prior proposals, we are proposing that senders and sender algorithms use the familiar pipeline syntax
for chaining, just like C++20 ranges do. Therefore, a user could write the following to represent a sequence of tasks to execute first on the CPU, then on a CUDA GPU, then on a CPU again, retrieving the result at the end:
[[
<pre highlight=c++>
auto [result] = execution::schedule(get_thread_pool())
    | execution::then([]{ return 123; })
    | execution::transfer(cuda::get_scheduler())
    | execution::then([](int i){ return 123 * 5; })
    | execution::transfer(get_thread_pool())
    | execution::then([](int i){ return i - 5; })
    | std::this_thread::sync_wait;
// result == 610
</pre>

## Senders are typed ## {#design-typed}

All senders must advertise the types they will [=send=] when they complete. This is necessary for a number of features, and writing code in a way that's agnostic of whether a predecessor is typed or not in common sender adapters such as `execution::then` is hard.
We are not aware of compelling use cases that *require* untyped senders (i.e. senders that effectively only decide what to send when you [=connect=] a receiver to them, and not without that), therefore we are proposing that all senders must be typed.

The mechanism for this advertisement is the same as in [[P0443R14]]; the way to query the types is through `sender_traits::value_types<tuple_like, variant_like>`.

### Design question: `sync_wait` ### {#design-typed-question1}

`sender_traits::value_types` is a template that takes two arguments: one is a tuple-like template, the other is a variant-like template. The tuple-like argument is required to represent senders sending more than one value (such as `when_all`). The variant-like
argument is required to represent senders that choose which specific values to send at runtime (such as `when_any`).

There's a choice made in the specification of [[#design-algorithm-sync_wait]]: it returns a tuple of values sent by the sender passed to it. However, this assumes that those values can be represented as a tuple, like here:

<pre highlight=c++>
execution::sender auto sends_1 = ...;
execution::sender auto sends_2 = ...;
execution::sender auto sends_3 = ...;

auto [a, b, c] = std::this_thread::sync_wait(
    execution::when_all(get_scheduler(sends_1),
        sends_1,
        sends_2,
        sends_3
    ));
// a == 1
// b == 2
// c == 3
</pre>

This works well for senders that don't require the variant-like argument to `sender_traits:;value_types`. However, if a sender requires the variant-like argument to represent all of its sent values, then the above doesn't work.

Issue: How should `sync_wait` handle senders that send multiple sets of types?

### Design question: canonical form ### {#design-typed-question2}

There's a similar problem to the above: what is the canonical form of `sender_traits::value_types`? Is there such a thing?

Specifically, given:

<pre highlight=c++>
execution::scheduler auto sched = get_thread_pool().scheduler();
execution::sender auto snd1 = execution::schedule(sched);
execution::sender auto snd2 = execution::then(snd1, []{ return 1; });

using type = execution::sender_traits<decltype(snd2)>::template value_types<
    std::tuple,
    std::variant
>;
</pre>

What are the permitted types `type` names?

The most straightforward answer to this question could be `int`, but that means that functions such as `sync_wait` need to add the tuple wrapper over the `int` to provide consistent interface for all senders (so that we can avoid breaking generic code like the
deduction guides for `std::tuple` did; if you saw `value_types` be `std;:tuple`, is that a wrapped tuple and you are supposed to treat it as the single value sent by the sender, or is it a tuple of the values sent by the sender?). It also makes implementing
`execution::then` harder (again, for the same reason - you can't tell the "single value that is a tuple" and "multiple values wrapped in a tuple" cases apart).

Okay, so maybe the answer should be `std::tuple<int>`. This may cause some inconvenience in unwrapping the tuple, but it should not be very problematic with structured bindings. But what about variants? Consider this example:

<pre highlight=c++>
execution::sender auto sends_int = ...;
execution::sender auto sends_float = ...;
execution::sender auto sends_string = ...;

execution::sender auto all = execution::when_any(
    get_scheduler(sends_int),
    sends_int,
    sends_float,
    sends_string
));

using type = execution::sender_traits<decltype(snd2)>::template value_types<
    std::tuple,
    std::variant
>;
</pre>

In this case we could allow senders to choose what they want to do, basically between:

 * `std::variant<int, float, std::string>`, or
 * `std::variant<std::tuple<int>, std::tuple<float>, std::tuple<std::string>>`

But this causes the same issues as earlier: if senders can decide, it's impossible to tell the difference between the second choice for the code above, and the first choice with senders that send tuples of single elements.

It seems that there *must* be a canonical form of `value_types` that all senders follow; and it seems that for a sender sending `Args1`, `Args2`, ..., `ArgsN` (where each `ArgsX` is a pack of types), the canonical form of `sender_traits<Tuple, Variant>` must be:

<pre highlight=c++>
Variant<
    Tuple<Args1...>,
    Tuple<Args2...>,
    ...,
    Tuple<ArgsN>
>
</pre>

However, this means that in the first example, with a relatively simple sender which senders precisely just a single integer every type, `value_types` must be `Variant<Tuple<int>>`. This seems suboptimal, but necessary for generic code to make sense (and not fall
prey of edge cases like `tuple(args...)` when `args` is a pack of a single element which is a tuple).

Issue: Is the above canonical form acceptable? Should it be required that all senders adhere to this canonical form of `sender_traits::value_types`? If not, what is the generic programming model for handling predecessors that use unknown forms of
`sender_traits::value_types`?

## Ranges-style CPOs vs `tag_invoke` ## {#design-dispatch}

XXX TODO: describe pros and cons of both; (MD) personally I think I'm starting to move into the `tag_invoke` camp, for better or worse

# Specification # {#spec}
